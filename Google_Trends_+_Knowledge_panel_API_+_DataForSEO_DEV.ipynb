{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/okLRFsqGwE5aX+yrGfm/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svetoslavseo/google-trends-automation/blob/main/Google_Trends_%2B_Knowledge_panel_API_%2B_DataForSEO_DEV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feedparser\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import os\n",
        "from xml.etree import ElementTree as ET\n",
        "import openai\n",
        "\n",
        "def fetch_google_trends_rss():\n",
        "    # URL for Google Trends RSS feed\n",
        "    google_trends_rss_url = \"https://trends.google.com/trending/rss?geo=GB\"\n",
        "\n",
        "    # Fetch the RSS feed as raw XML\n",
        "    response = requests.get(google_trends_rss_url)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Failed to fetch the feed.\")\n",
        "        return None\n",
        "    raw_feed = response.content\n",
        "\n",
        "    # Parse the RSS feed\n",
        "    feed = feedparser.parse(raw_feed)\n",
        "\n",
        "    # Check if the feed was parsed successfully\n",
        "    if not feed.entries:\n",
        "        print(\"Failed to fetch the feed or no entries available.\")\n",
        "        return None\n",
        "\n",
        "    # Parse the raw XML for extracting <ht:news_item_title>\n",
        "    root = ET.fromstring(raw_feed)\n",
        "\n",
        "    # Extract the data into a list of dictionaries\n",
        "    trends_data = []\n",
        "    max_titles_count = 0  # Track the maximum number of news titles per item for dynamic column creation\n",
        "    for entry in feed.entries:\n",
        "        # Convert published time to UK time\n",
        "        utc_time = datetime.strptime(entry.published, \"%a, %d %b %Y %H:%M:%S %z\")\n",
        "        uk_time = utc_time.astimezone(pytz.timezone(\"Europe/London\"))\n",
        "\n",
        "        # Extract news titles under <ht:news_item> for the current <item>\n",
        "        news_titles = []\n",
        "        for item in root.findall(\".//item\"):\n",
        "            item_title = item.find(\"title\").text\n",
        "            if item_title == entry.title:\n",
        "                for news_item in item.findall(\".//{https://trends.google.com/trending/rss}news_item\"):\n",
        "                    news_title = news_item.find(\"{https://trends.google.com/trending/rss}news_item_title\")\n",
        "                    if news_title is not None:\n",
        "                        news_titles.append(news_title.text)\n",
        "                break\n",
        "\n",
        "        max_titles_count = max(max_titles_count, len(news_titles))  # Update maximum title count\n",
        "\n",
        "        # Base trend information\n",
        "        trend_info = {\n",
        "            \"title\": entry.title,\n",
        "            \"published\": uk_time.strftime(\"%Y-%m-%d %H:%M:%S %Z\"),\n",
        "            \"summary\": entry.summary,\n",
        "            \"link\": entry.link,\n",
        "            \"approx_traffic\": getattr(entry, 'ht_approx_traffic', 'N/A'),\n",
        "            \"picture_url\": getattr(entry, 'ht_picture', 'N/A'),\n",
        "            \"picture_source\": getattr(entry, 'ht_picture_source', 'N/A')\n",
        "        }\n",
        "\n",
        "        # Add dynamic columns for news titles\n",
        "        for i, news_title in enumerate(news_titles):\n",
        "            trend_info[f\"news_title_{i + 1}\"] = news_title\n",
        "\n",
        "        trends_data.append(trend_info)\n",
        "\n",
        "    # Convert the list of dictionaries into a DataFrame\n",
        "    df = pd.DataFrame(trends_data)\n",
        "\n",
        "    # Fill missing dynamic columns with None if some rows have fewer titles\n",
        "    for i in range(1, max_titles_count + 1):\n",
        "        if f\"news_title_{i}\" not in df.columns:\n",
        "            df[f\"news_title_{i}\"] = None\n",
        "\n",
        "    return df\n",
        "\n",
        "def categorize_with_knowledge_graph(keyword):\n",
        "    api_key = \"<YOUR-GOOGLE-KNOWLEDGE-GRAPH-API-KEY\"  # Replace with your Google Knowledge Graph API key\n",
        "    url = f\"https://kgsearch.googleapis.com/v1/entities:search?query={keyword}&key={api_key}&limit=1\"\n",
        "    response = requests.get(url).json()\n",
        "\n",
        "    if 'itemListElement' in response and response['itemListElement']:\n",
        "        entity = response['itemListElement'][0].get('result', {})\n",
        "        return entity.get('description', 'Unknown'), entity.get('@type', ['Unknown'])\n",
        "    return 'Unknown', ['Unknown']\n",
        "\n",
        "def check_serp_for_top_stories_and_telegraph(keyword):\n",
        "    # Replace with your DataForSEO API credentials\n",
        "    username = \"<USER-NAME\"\n",
        "    password = \"<PASSWORD\"\n",
        "    url = \"https://api.dataforseo.com/v3/serp/google/organic/live/advanced\"\n",
        "\n",
        "    payload = {\n",
        "        \"keyword\": keyword,\n",
        "        \"location_code\": 2840,  # United Kingdom location code\n",
        "        \"language_code\": \"en\",\n",
        "        \"device\": \"mobile\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, auth=(username, password), json=[payload])\n",
        "    response_json = response.json()\n",
        "\n",
        "    # Check for errors\n",
        "    if 'tasks' in response_json and response_json['tasks']:\n",
        "        task = response_json['tasks'][0]\n",
        "        if task['result'] and task['result'][0]['items']:  # Check for SERP results\n",
        "            serp_items = task['result'][0]['items']\n",
        "            top_stories_found = \"False\"\n",
        "            telegraph_in_top_stories = \"False\"\n",
        "            telegraph_in_organic = \"False\"\n",
        "\n",
        "            for item in serp_items:\n",
        "                if item.get('type') == 'top_stories':\n",
        "                    top_stories_found = \"True\"\n",
        "                    for story in item.get('items', []):\n",
        "                        if 'www.telegraph.co.uk' in story.get('url', ''):\n",
        "                            telegraph_in_top_stories = \"True\"\n",
        "                            break\n",
        "\n",
        "                elif item.get('type') == 'organic':  # Check organic results\n",
        "                    if 'www.telegraph.co.uk' in item.get('url', ''):\n",
        "                        telegraph_in_organic = \"True\"\n",
        "\n",
        "            return top_stories_found, telegraph_in_top_stories, telegraph_in_organic\n",
        "    return \"False\", \"False\", \"False\"\n",
        "\n",
        "def main():\n",
        "    # Fetch Google Trends data\n",
        "    df = fetch_google_trends_rss()\n",
        "\n",
        "    if df is not None:\n",
        "        output_file = \"google_trends_with_categories.csv\"\n",
        "\n",
        "        # Check if file exists\n",
        "        if os.path.exists(output_file):\n",
        "            # Load existing data\n",
        "            existing_df = pd.read_csv(output_file)\n",
        "            # Append new data\n",
        "            df = pd.concat([existing_df, df], ignore_index=True)\n",
        "\n",
        "        # Remove duplicate titles, keeping the first occurrence\n",
        "        df = df.drop_duplicates(subset=['title'], keep='first')\n",
        "\n",
        "        # Add categories using Google Knowledge Graph API\n",
        "        print(\"Fetching categories using Google Knowledge Graph API...\")\n",
        "        df[['Category', 'EntityType']] = df['title'].apply(\n",
        "            lambda x: pd.Series(categorize_with_knowledge_graph(x))\n",
        "        )\n",
        "\n",
        "        # Check SERP for Top Stories and Telegraph\n",
        "        print(\"Checking SERP for Top Stories and Telegraph...\")\n",
        "        df[['Top_Stories', 'Telegraph_in_Top_Stories','Telegraph_in_Organic_Results']] = df['title'].apply(\n",
        "            lambda x: pd.Series(check_serp_for_top_stories_and_telegraph(x))\n",
        "        )\n",
        "\n",
        "        # Save the cleaned and updated DataFrame back to the file\n",
        "        df.to_csv(output_file, index=False)\n",
        "\n",
        "        # Display the DataFrame in a table format\n",
        "        print(\"\\nGoogle Trends Data with Summaries:\\n\")\n",
        "        print(df.to_markdown(index=False))\n",
        "\n",
        "        print(f\"\\nData has been saved to '{output_file}'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "df_2 = pd.read_csv(\"google_trends_with_categories.csv\")\n",
        "# Remove the '+' character from the approx_traffic column\n",
        "df_2['approx_traffic'] = df_2['approx_traffic'].str.replace('+', '', regex=False)\n",
        "\n",
        "# Drop the 'summary' column\n",
        "df_2 = df_2.drop(columns=['summary'])\n",
        "\n",
        "# Optionally, save the cleaned data back to a CSV\n",
        "df_2.to_csv('cleaned_file.csv', index=False)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "\n",
        "\n",
        "# Create DataFrame\n",
        "df_3 = df_2 = pd.read_csv(\"cleaned_file.csv\")\n",
        "\n",
        "# Combine news titles into a single text column for topic modeling\n",
        "df_3[\"combined_news_titles\"] = df_3[\"news_title_1\"] + \" \" + df_3[\"news_title_2\"] + \" \" + df_3[\"news_title_3\"]\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_texts = [doc.split() for doc in df_3[\"combined_news_titles\"]]\n",
        "\n",
        "# Create dictionary and corpus for LDA\n",
        "dictionary = Dictionary(tokenized_texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
        "\n",
        "# Train LDA model with 4 topics\n",
        "lda_model = LdaModel(corpus, num_topics=4, id2word=dictionary, passes=10)\n",
        "\n",
        "# Get the topic distribution for each document\n",
        "df_3[\"LDA_Topic\"] = [\n",
        "    max(lda_model.get_document_topics(dictionary.doc2bow(doc)), key=lambda x: x[1])[0]\n",
        "    for doc in tokenized_texts\n",
        "]\n",
        "\n",
        "# Display topics\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "\n",
        "# Print the topics\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "\n",
        "# Show the results\n",
        "df_3.to_csv(\"lda_categorized_news.csv\", index=False)  # Save results to CSV\n",
        "\n",
        "\n",
        "#Topic 0: Likely related to travel, with keywords like Top, 5, beach, destinations.\n",
        "#Topic 1: Focuses on sports, with keywords like wins, Champions, title, Manchester, final.\n",
        "#Topic 2: Seems related to economy/finance, including words like Stock, markets, projected.\n",
        "#Topic 3: Strongly connected to technology, featuring AI, new, Tech, companies, revolutionizing.\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "from google.colab import auth\n",
        "\n",
        "# Authenticate and set up OpenAI API key\n",
        "auth.authenticate_user()\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<REPLACE-WITH-YOUR-OPENAI-API-KEY\"  # Set your OpenAI API key\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Ensure API key is set\n",
        ")\n",
        "\n",
        "# Load the dataset\n",
        "df_4 = pd.read_csv(\"lda_categorized_news.csv\")\n",
        "\n",
        "# Ensure the required column exists\n",
        "df_4[\"combined_news_titles\"] = df_4.get(\"news_title_1\", \"\") + \" \" + df_4.get(\"news_title_2\", \"\") + \" \" + df_4.get(\"news_title_3\", \"\") + df_4.get(\"Category\", \"\")\n",
        "\n",
        "# Function to classify news using OpenAI API\n",
        "def classify_news_department(news_titles):\n",
        "    if not client.api_key:\n",
        "        raise ValueError(\"OpenAI API key is not set. Please set the OPENAI_API_KEY environment variable.\")\n",
        "\n",
        "    prompt = f\"\"\"Based on the following news headlines and the category from Google Knowledge Panel at the end:\n",
        "    {news_titles}\n",
        "\n",
        "    Which department does this news belong to? Choose from:\n",
        "    - News (Politics, Business, Economy, General)\n",
        "    - Royal Family\n",
        "    - Sports\n",
        "    - Entertainment\n",
        "    - Technology\n",
        "    - Science & Health\n",
        "    - Travel\n",
        "    - Lifestyle\n",
        "\n",
        "    Return only the department name.\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a news classification expert.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Function to classify news using OpenAI API\n",
        "def title_summary(news_titles):\n",
        "    if not client.api_key:\n",
        "        raise ValueError(\"OpenAI API key is not set. Please set the OPENAI_API_KEY environment variable.\")\n",
        "\n",
        "    prompt = f\"\"\"Based on the provided news title, write a short summary of the titles by mentioning the main topic.\n",
        "    {news_titles}\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert in summarising news in a newsparer.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Apply summary to each row\n",
        "df_4[\"News_Summary\"] = df_4[\"combined_news_titles\"].apply(lambda x: title_summary(x) if x.strip() else \"Unknown\")\n",
        "\n",
        "# Apply classification to each row\n",
        "df_4[\"News_Department\"] = df_4[\"combined_news_titles\"].apply(lambda x: classify_news_department(x) if x.strip() else \"Unknown\")\n",
        "\n",
        "# Save results to Google Drive or Colab environment, keeping all original columns\n",
        "df_4.to_csv(\"openai_categorized_news.csv\", index=False)\n",
        "\n",
        "print(df_4.to_markdown(index=False))\n",
        "\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
        "\n",
        "# Open the Google Sheet\n",
        "sh = gc.open_by_key('<GOOGLE-SPREADSHEET-KEY>')  # Google Sheet URL key\n",
        "worksheet = sh.worksheet(\"Sheet1\")\n",
        "\n",
        "# Load existing data from the sheet into a DataFrame\n",
        "existing_df = get_as_dataframe(worksheet, evaluate_formulas=True).dropna(how='all')\n",
        "\n",
        "# Load the new data\n",
        "new_df = pd.read_csv(\"openai_categorized_news.csv\")  # Read from the CSV file\n",
        "\n",
        "# Append new data to existing data\n",
        "if not existing_df.empty:\n",
        "    combined_df = pd.concat([existing_df, new_df], ignore_index=True).drop_duplicates(subset=['title'], keep='first')\n",
        "else:\n",
        "    combined_df = new_df\n",
        "\n",
        "# Write the updated DataFrame back to the Google Sheet\n",
        "worksheet.clear()  # Clear existing data on the sheet\n",
        "set_with_dataframe(worksheet, combined_df)  # Write the combined data\n",
        "\n",
        "print(\"Data has been successfully appended to the Google Sheet.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPi8_o3IdzKz",
        "outputId": "f60a77f2-998f-4412-9ac0-f6fa24f53640"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=418597bdb61a37721ce7703a3e70dc0b71c4fe4f89d0e0a02616b595e2411a53\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "krWPtt5s3TZI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}